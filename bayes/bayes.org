#+TITLE: Bayesian statistics and data analysis


TODO: analysis from paper with safa

* Notes

** Topics to cover

*** Conceptual difference between frequentist and bayesian inference

(((I think this is going to take the most time...)

long-run behavior vs. parameter inference, bayes rule.  

p value, (why not) confidence intervals

bayes rule/inference: in order to make the interpretation that you want you need
to specify prior in order to make it work.

example: t-test

*** Generative modeling

formulating a data analysis question in terms of a generative
model...generative models can be descriptive or causal or anywhere in between.

the goal of the generative model is for you to be able to calculate the
likelihood of some observed data, given a particular value of your parameters

example: ab-yesno analysis?

*** working with samples in place of distributions

how do you actually DO this??  analytic vs. sampling (could use the stuff from
the kavli summer school maybe, and/or from ideal adapter slides?)  can at least
do the t test example, and show how it includes teh variance too?

importance sampling (re-weighting your hypotheses in the face of new evidence)

MCMC: if you have one sample, can you generate another one?

*** model comparison?

optional I guess: it's a tricky thing to communicate clearly...but probably
important to understand...



* Frequentist vs. Bayesian thinking (conceptual stuff)

Frequentist and bayesian perspectives on probability are almost completely
opposite.
  
From a frequentist perspective, probabilities are long-run replications of a
fixed random process.  The goal is to find some procedure to *estimate* some
unknown-but-fixed parameter.

From a Bayesian perspective, probabilities represent *uncertain knowledge* about
some un-observed thing.  The goal is to *update your beliefs* based on
evidence.

** 

* The generative model

Bayesian analysis starts with a "generative model", a quantitative model that
links the *parameters* that you don't know, with the *observations* that you
do.

** ACTIVITY
think of a data analysis problem you've had, and think about how you'd frame it
as a generative model...

* moves and maneuvers

** Prior and posterior

Want to go from p(data | parameters) (generative model) to p(parameters | data).
How can we do that??  Basic idea is to use an axiom of probability theory:

p(X,Y) = p(X | Y)p(Y)

This means: ...

We just as well write this as p(X,Y) = p(Y | X)p(X), which means we have this
equality:

p(Y | X) p(X) = p(X | Y) p(Y)

If we say that X is our data, and Y is our parameters, then we can get the
*posterior probability* as

p(Y | X) = p(X | Y) p(Y) / p(X)
  
** Marginalization

The other important thing we migth want to do with probabilities is to make some
kind of inference taking into account our full uncertainty about unobserved
variables.  For instance...

* Inference

How do you actually DO inference?  For some models, you can write down the
relationship between the prior, data, and posterior directly.  But that is
rarely the case and it takes some heavy math lifting even in the few cases that
it can work.

YOu might say, how can this be?  You just multiply the posterior probability
times the prior probability and call it good.  Two problems: 

1. You have to do this for every possible value of the parameters
2. You have to make sure that the posterior probabilities add up to 1 (because
   that's what makes a probability distribution a probability distribution).

Together these make computing the posterior REALLY HARD, unless you have some
math trick which allows you to easily calculate the posterior

** Sampling techniques

Instead, most techniques don't work with the posterior distribution DIRECTLY.

* Model comparison

Issue: inferences are always conditional on the generative model itself.

Solution: model comparison?  We can put a prior probability on different models
right?  And compare how well they match the data?

Yes and no.  There are two issues, one *practical* and one *conceptual*

** Practical: marginalizing the parameters

If you want to compute how well a model explains data, you have to be very
careful.  The basic idea is exactly the same as doing inference about
parameters: you compute the likelihood of the data given the model.  But how do
we compute the likelihood of the data given the model?  Which parameter values
should we use?  

The answer is that if you want p(model | data), you need to compute p(data |
model), not p(data | model, parametrs).  And computing p(data | model) requires
averaging over all the possible settings of the parameters, *weighted by their
prior probability*:

p(data | model) = âˆ«p(data | model, parameters) p(parameters | model) d
parameters

Because this depends on the *prior probability* of the parameters, model
comparison is HIGHLY sensitive to the choice of prior.  In a way that's not true
for parameter estimation, where the posterior is dominted by the data except
when there isn't much data.

It's also computationally really hard: the part of the model's parameter space
that provides reasonably good fit to the data is likely very small, so all the
usual sampling tricks are likely to completely miss those parameters giving you
a massively distorted picture of how good the model is overall.  And if you can
use sampling then you have to come up with some other clever way of exploring
the parameter space, which is really hard.

*** (why marginalize?)

A spiritual question: why not use the parameter values that best fit the data.

Problem with this is that a very complex, clever model will probably have some
parameter settings which explain the observed data very, very well.  But those
parameters may not *generalize well* to other datasets.

** Conceptual: what is the true model?

Even if you overcome all the technical challenges with 
   
The inferences about which model is best is only valid insofar as the set of
models you are considering contains the true model, or more loosely contains a
good-enough model.  But there's no way to know that: a good posterior
probability for a model only tells you that it's better than the alternatives
you're considering, not 

* Wrapping up

So why bother with all this??  Analysis still rests on a lot of assumptions, so
it doesn't seem to be buying us anything over 'standard' frequentist analyses.

Two advantages: you are "allwoed" to make the inferences you are tempted to make
based on frequentist analyses but cannot.  There are always assumptions!

Assumptions have to be explicitly stated and justified



